{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0476c8f-d074-4050-901f-5aed4d18d18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h2>Scikit-learn steps</h2>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<h2>Scikit-learn steps</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51949107-7a13-40b9-895f-6960e9b1419b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "<tr>\n",
       "<th>No.</th>\n",
       "<th>Steps</th>\n",
       "<th>Description</th>\n",
       "<th>function</th>\n",
       "</tr>\n",
       " \n",
       "<tr>\n",
       "<td>\n",
       " 1. \n",
       "</td>\n",
       " <td>\n",
       " Get the data & visualise the basic statistics\n",
       "</td>\n",
       " <td>\n",
       " Mainly used to understand the data\n",
       "</td>\n",
       " <td>\n",
       " Pandas has built in visulaization. Also, Matplotlib is used. \n",
       "</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td>\n",
       " 2. \n",
       "</td>\n",
       " <td>\n",
       " Create a train-test dataset & label\n",
       "</td>\n",
       " <td>\n",
       "Required for supervised & semi-supervised problem\n",
       "</td>\n",
       " <td>\n",
       "sklearn.<b>model_selection</b> import <b>train_test_split</b>\n",
       "<br/>\n",
       "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) \n",
       "<hr/>\n",
       "strat_train_set, strat_test_set = train_test_split(housing, test_size=0.2, random_state=42, <b>stratify=housing['income_cat']</b>)\n",
       "</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td>\n",
       " 3. \n",
       "</td>\n",
       " <td>\n",
       " Explore & visualize to gain insight from the data\n",
       "</td>\n",
       " <td>\n",
       " Look for correlation by visualizing it\n",
       "</td>\n",
       " <td>\n",
       " pandas has built in visulaization.\n",
       "</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td>\n",
       " 4. \n",
       "</td>\n",
       " <td>\n",
       " Clean the Data\n",
       "</td>\n",
       " <td>\n",
       "How to work with missing features. For example, total_bedrooms feature\n",
       "<br/>\n",
       "1. Get rid of the entire row which has missing value in the total_bedrooms column\n",
       "<br/>\n",
       "2. Get rid of the whole feature (column) -> drop total_bedrooms\n",
       "<br/>\n",
       "3. Set the the missing value (Zero, the mean, the median, etc.). This is called imputation.\n",
       "</td>\n",
       " <td>\n",
       "Option 1 & 2 are mainly done using panda NaN functions. E.g. df.dropna(), df.drop, df.fillna()\n",
       "<hr/>\n",
       "Option 3 can be done with pandas or imputer function from the sklearn.\n",
       "from sklearn.<b>impute</b> import <b>SimpleImputer</b>\n",
       "<br/>\n",
       "imputer = SimpleImputer(strategy=\"median\")\n",
       "<br/>\n",
       "housing_num = <b>housing.select_dtypes(include=[np.number])</b>\n",
       "<br/>\n",
       "X = imputer.<b>fit_transform</b>(housing_num)\n",
       "</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td>\n",
       " 5. \n",
       "</td>\n",
       " <td>\n",
       "Handling Text and Categorical Attributes/Features\n",
       "</td>\n",
       " <td>\n",
       "OrdinalEncoder and OneHotEncoder from sklearn\n",
       "</td>\n",
       " <td>\n",
       "housing_cat = housing[['ocean_proximity']]\n",
       "<br/>\n",
       "from sklearn.<b>preprocessing</b> import <b>OneHotEncoder</b>\n",
       "<br/>\n",
       "onehot_encoder = OneHotEncoder()\n",
       "<br/>\n",
       "housing_cat_encoded = onehot_encoder.fit_transform(housing_cat)\n",
       "</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td>\n",
       " 6\n",
       "</td>\n",
       " <td>\n",
       "Feature Scaling & Transformation\n",
       "</td>\n",
       " <td>\n",
       "    <br/>\n",
       "There are two approaches to feature scaling:<b> min-max scaling and standardization</b>\n",
       "<br/>\n",
       "<b>min-max</b> scales the data ranging between minimum to maximum we define\n",
       "<hr/>\n",
       "<b>Standardization</b> is done by subtracting the value from the mean and dividing it by SD. \n",
       "It\\'s less affected by outliers, however, it won't restrict the values between certain ranges.\n",
       "<hr/>\n",
       "Both approaches will not work for feature distribution that has heavy tail. The solution would be replacing the feature with it\\'s logarithm,\n",
       "bucketizing the feature, etc.\n",
       "<hr/>\n",
       "As we are transforming the data,  we need to reverse the transformation to get the actual values\n",
       "</td>\n",
       " <td>\n",
       " from sklearn.preprocessing import MinMaxScaler\n",
       "<br/>\n",
       "min_max_scaler = <b>MinMaxScaler(feature_range=(-1,1))</b>\n",
       "<br/>\n",
       "housing_num_min_max_scaled = min_max_scaler.<b>fit_transform(housing_num)</b>\n",
       "<hr/>\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "<br/>\n",
       "std_scaler = StandardScaler()\n",
       "<br/>\n",
       "housing_num_std_scaled =  std_scaler.fit_transform(housing_num)\n",
       "<hr/>\n",
       "target_scaler = StandardScaler()\n",
       "<br/>\n",
       "...\n",
       "<br/>\n",
       "...\n",
       "<br/>\n",
       "scaled_predictions = model.predict(some_new_data)\n",
       "<br/>\n",
       " predictions = traget_scaler.<b>inverse_transform</b>(scaled_predictions)\n",
       "</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td>\n",
       " 7 \n",
       "</td>\n",
       " <td>\n",
       " Custom transformer (optional)\n",
       "</td>\n",
       " <td>\n",
       " This is an optional step where feature scaling with standard transformers is not sufficient.\n",
       "<hr>\n",
       " We can create a basic custom transformer using the function transformer. \n",
       " For replacing heavy-tailed distribution with its logarithm, we can use the function transformer.\n",
       "<hr>\n",
       "If we would like to tranformer to be trainable: learning some parameter in the fit() and using them later in the transform() just like other standard tranformers,\n",
       "We need to write a custom class\n",
       "TransformerMixin parent class brings the fir_transform method so we do not need to specifically write here\n",
       "\n",
       "</td>\n",
       " <td>\n",
       "from sklearn.preprocessing import FunctionTransformer\n",
       "<br/>\n",
       "log_transformer = <b>FunctionTransformer(np.log, inverse_func=np.exp)</b> \n",
       "<br/>\n",
       "log_transformer.transform(housing['population'])\n",
       "<hr>\n",
       "\n",
       "from sklearn.base import BaseEstimator, TransformerMixin\n",
       "<br/>\n",
       "from sklearn.utils.validation import check_array, check_is_fitted\n",
       "<br/>\n",
       "<br/>\n",
       "<b>class custom_class_name(BaseEstimator, TransformerMixin):</b>\n",
       "<br/>\n",
       "<br/>\n",
       "    def __init__(self, with_mean=True):\n",
       "        <br/>   \n",
       "        self.with_mean=True\n",
       "<br/>\n",
       "<br/>\n",
       "    <b>def fit(self, X, y=None):</b>\n",
       "        <br/>   \n",
       "        return self \n",
       "<br/>\n",
       "<br/>\n",
       "    <b>def transform(self, X):</b>\n",
       "        <br/>   \n",
       "        return transformed_data\n",
       "<br/>   \n",
       "<br/>  \n",
       "custom_class_name_clone = custom_class_name()\n",
       "<br/>\n",
       "<br/>\n",
       "scaled = custom_class_name_clone.fit_transform(df)\n",
       "</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td>\n",
       " 8\n",
       "</td>\n",
       " <td>\n",
       "Transformation pipelines\n",
       "</td>\n",
       " <td>\n",
       " Scikit-learn provides the pipeline class to help with sequence of transformations, mainly for numerical data.\n",
       "<hr>\n",
       "So far we've dealt with numerical & categorical features separately. We can combine them using the ColumnTransformer class \n",
       "</td>\n",
       " <td>\n",
       "from sklearn.pipeline import Pipeline\n",
       "<br/>\n",
       "<b>num_pipeline = Pipeline([</b>\n",
       "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
       "    (\"standardize\", StandardScaler()),\n",
       "])\n",
       "<br/>\n",
       "housing_num_prepared = num_pipeline.<b>fit_transform(housing_num)</b>\n",
       "<br/>\n",
       "df_test = pd.DataFrame(housing_num_prepared, columns=num_pipeline.get_feature_names_out(), index = housing_num.index)\n",
       "<hr>\n",
       "\n",
       "from sklearn.pipeline import Pipeline\n",
       "<br/>\n",
       "from sklearn.preprocessing import OneHotEncoder\n",
       "<br/>\n",
       "from sklearn.compose import ColumnTransformer\n",
       "<br/>\n",
       "<br/>\n",
       "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
       "               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
       "<br/>\n",
       "<br/>\n",
       "cat_attribs = [\"ocean_proximity\"]\n",
       "<br/>\n",
       "<br/>\n",
       "<b>num_pipeline = Pipeline</b>([\n",
       "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
       "    (\"standardize\", StandardScaler())\n",
       "])\n",
       "<br/>\n",
       "<br/>\n",
       "<b>cat_pipeline = Pipeline</b>([\n",
       "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
       "    (\"one_hot_encoding\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
       "])\n",
       "<br/>\n",
       "<br/>\n",
       "<b>preprocessing = ColumnTransformer</b>([\n",
       "    (\"num\", num_pipeline, num_attribs),\n",
       "    (\"cat\", cat_pipeline, cat_attribs)\n",
       "])\n",
       "<br/>\n",
       "<br/>\n",
       "housing_prepared = <b>preprocessing.fit_transform(housing)</b>\n",
       "\n",
       "</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td>\n",
       " 9 \n",
       "</td>\n",
       " <td>\n",
       "Train a model by adding to the training pipeline & calling the fit method\n",
       "</td>\n",
       " <td>\n",
       "We already have the trnasformation pipeline and we can add model training to the pipeline. Then fir on the data\n",
       "</td>\n",
       " <td>\n",
       "\n",
       "<b>tree_reg = Pipeline([</b>\n",
       "    ('preprocessing', preprocessing),\n",
       "    ('decision_tree_regressor',DecisionTreeRegressor(random_state=42))\n",
       "])\n",
       "<b>tree_reg.fit</b>(housing, housing_labels)\n",
       "</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td>\n",
       " 10\n",
       "</td>\n",
       " <td>\n",
       "Efficient evaluation using cross validation (Optinal)\n",
       "</td>\n",
       " <td>\n",
       "We can better evaluate the model using k fold cross validation. In this, training set is randomly splits into 10 nooverlpaping folds is k=10.\n",
       "Then it train & evalaute the decision tree model 10 times, picking a different fold for evaluation every time and using the other 9 folds for training. \n",
       "The result is an array containing the 10 evaluation scores.\n",
       "<img src='images/cross_validation.png' width=250/>\n",
       "\n",
       "</td>\n",
       " <td>\n",
       "from sklearn.model_selection import cross_val_score\n",
       "<br/>\n",
       "tree_rmse = <b>-cross_val_score</b>(tree_reg, housing, housing_labels, scoring=\"neg_root_mean_squared_error\", <b>cv=10</b>)\n",
       "<br/>\n",
       "</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td>\n",
       " 11 \n",
       "</td>\n",
       " <td>\n",
       " Finetune the model (Optional)\n",
       "</td>\n",
       " <td>\n",
       " Normally we need to manually tune the hyperparameters to get the best fit model. Scikit-learn provides tuning methods along with cross-validation: Grid search cv and randomized search cv\n",
       "GridsearchCV is used when the search space is realtively small and randomizedSearchCV is preferred when the search space is large or continuous.\n",
       "\n",
       "    </td>\n",
       " <td>\n",
       "from sklearn.model_selection import GridSearchCV\n",
       "<br/>\n",
       "from sklearn.ensemble import RandomForestRegressor\n",
       "<br/>\n",
       "<br/>\n",
       "<b>full_pipeline = Pipeline([\n",
       "    ('preprocessing', preprocessing),\n",
       "    ('random_forest', RandomForestRegressor(random_state=42))\n",
       "])</b>\n",
       "<br/>\n",
       "<br/>\n",
       "# Hyperparameters of random_forest: search space has only two items\n",
       "<br/>\n",
       "<b>param_grid = [</b>\n",
       "    {'random_forest__max_features': [4, 6, 8]}\n",
       "]\n",
       "<br/>\n",
       "<br/>\n",
       "grid_search = <b>GridSearchCV</b>(full_pipeline, param_grid, cv=3, scoring='neg_root_mean_squared_error')\n",
       "<br/>\n",
       "<br/>\n",
       "grid_search.<b>fit</b>(housing, housing_labels)\n",
       "\n",
       "<hr>\n",
       "\n",
       "from sklearn.model_selection import RandomizedSearchCV\n",
       "<br/>\n",
       "from scipy.stats import randint \n",
       "<br/>\n",
       "<br/>\n",
       "<b>full_pipeline = Pipeline([\n",
       "    ('preprocessing', preprocessing),\n",
       "    ('random_forest', RandomForestRegressor(random_state=42))\n",
       "])</b>\n",
       "<br/>\n",
       "<br/>\n",
       "<b>param_distribs = {'random_forest__max_features': randint(low=2, high=4)}</b>\n",
       "<br/>\n",
       "<br/>\n",
       "rnd_search = <b>RandomizedSearchCV(</b>\n",
       "    full_pipeline, <b>param_distributions=param_distribs, n_iter=1, </b>cv=3,\n",
       "    scoring='neg_root_mean_squared_error', random_state=42)\n",
       "<br/>\n",
       "<br/>\n",
       "rnd_search.fit(housing, housing_labels)\n",
       "</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td>\n",
       " 12\n",
       "</td>\n",
       " <td>\n",
       " Analysing the best models and their errors (Optional)\n",
       "</td>\n",
       " <td>\n",
       "We can analyse the feature's importance so that we can drop and keep the features which are less or more important.\n",
       "</td>\n",
       " <td>\n",
       "final_model = rnd_search.best_estimator_\n",
       "<br/>\n",
       "<br/>\n",
       "feature_importances = final_model[\"random_forest\"].feature_importances_\n",
       "<br/>\n",
       "<br/>\n",
       "sorted(zip(feature_importances, final_model['preprocessing'].get_feature_names_out()), reverse=True)\n",
       "</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr>\n",
       "<td>\n",
       " 13\n",
       "</td>\n",
       " <td>\n",
       " Evaluate your system on the test set\n",
       "</td>\n",
       " <td>\n",
       "It helps to identify how the system will perform on unseen data.\n",
       "</td>\n",
       " <td>\n",
       "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
       "<br/>\n",
       "<br/>\n",
       "y_test = strat_test_set[\"median_house_value\"].copy()\n",
       "<br/>\n",
       "<br/>\n",
       "final_predictions = final_model.predict(X_test)\n",
       "<br/>\n",
       "<br/>\n",
       "final_rmse = mean_squared_error(y_test, final_predictions, squared=False)\n",
       "print(final_rmse)\n",
       "</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td>\n",
       " 14\n",
       "</td>\n",
       " <td>\n",
       " Saving and loading the model\n",
       "</td>\n",
       " <td>\n",
       "\n",
       "</td>\n",
       " <td>\n",
       "import joblib\n",
       "<br/>\n",
       "    <br/>\n",
       "# Save the final model\n",
       "    <br/>\n",
       "joblib.dump(final_model, \"my_california_housing_model.pkl\")\n",
       "    <br/>\n",
       "    <br/>\n",
       "\n",
       "# Load the saved model\n",
       "    <br/>\n",
       "final_model_reloaded = joblib.load(\"my_california_housing_model.pkl\")\n",
       "</td>\n",
       "</tr>\n",
       "\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>No.</th>\n",
    "<th>Steps</th>\n",
    "<th>Description</th>\n",
    "<th>function</th>\n",
    "</tr>\n",
    " \n",
    "<tr>\n",
    "<td>\n",
    " 1. \n",
    "</td>\n",
    " <td>\n",
    " Get the data & visualise the basic statistics\n",
    "</td>\n",
    " <td>\n",
    " Mainly used to understand the data\n",
    "</td>\n",
    " <td>\n",
    " Pandas has built in visulaization. Also, Matplotlib is used. \n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    " 2. \n",
    "</td>\n",
    " <td>\n",
    " Create a train-test dataset & label\n",
    "</td>\n",
    " <td>\n",
    "Required for supervised & semi-supervised problem\n",
    "</td>\n",
    " <td>\n",
    "sklearn.<b>model_selection</b> import <b>train_test_split</b>\n",
    "<br/>\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) \n",
    "<hr/>\n",
    "strat_train_set, strat_test_set = train_test_split(housing, test_size=0.2, random_state=42, <b>stratify=housing['income_cat']</b>)\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    " 3. \n",
    "</td>\n",
    " <td>\n",
    " Explore & visualize to gain insight from the data\n",
    "</td>\n",
    " <td>\n",
    " Look for correlation by visualizing it\n",
    "</td>\n",
    " <td>\n",
    " pandas has built in visulaization.\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    " 4. \n",
    "</td>\n",
    " <td>\n",
    " Clean the Data\n",
    "</td>\n",
    " <td>\n",
    "How to work with missing features. For example, total_bedrooms feature\n",
    "<br/>\n",
    "1. Get rid of the entire row which has missing value in the total_bedrooms column\n",
    "<br/>\n",
    "2. Get rid of the whole feature (column) -> drop total_bedrooms\n",
    "<br/>\n",
    "3. Set the the missing value (Zero, the mean, the median, etc.). This is called imputation.\n",
    "</td>\n",
    " <td>\n",
    "Option 1 & 2 are mainly done using panda NaN functions. E.g. df.dropna(), df.drop, df.fillna()\n",
    "<hr/>\n",
    "Option 3 can be done with pandas or imputer function from the sklearn.\n",
    "from sklearn.<b>impute</b> import <b>SimpleImputer</b>\n",
    "<br/>\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "<br/>\n",
    "housing_num = <b>housing.select_dtypes(include=[np.number])</b>\n",
    "<br/>\n",
    "X = imputer.<b>fit_transform</b>(housing_num)\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    " 5. \n",
    "</td>\n",
    " <td>\n",
    "Handling Text and Categorical Attributes/Features\n",
    "</td>\n",
    " <td>\n",
    "OrdinalEncoder and OneHotEncoder from sklearn\n",
    "</td>\n",
    " <td>\n",
    "housing_cat = housing[['ocean_proximity']]\n",
    "<br/>\n",
    "from sklearn.<b>preprocessing</b> import <b>OneHotEncoder</b>\n",
    "<br/>\n",
    "onehot_encoder = OneHotEncoder()\n",
    "<br/>\n",
    "housing_cat_encoded = onehot_encoder.fit_transform(housing_cat)\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    " 6\n",
    "</td>\n",
    " <td>\n",
    "Feature Scaling & Transformation\n",
    "</td>\n",
    " <td>\n",
    "    <br/>\n",
    "There are two approaches to feature scaling:<b> min-max scaling and standardization</b>\n",
    "<br/>\n",
    "<b>min-max</b> scales the data ranging between minimum to maximum we define\n",
    "<hr/>\n",
    "<b>Standardization</b> is done by subtracting the value from the mean and dividing it by SD. \n",
    "It\\'s less affected by outliers, however, it won't restrict the values between certain ranges.\n",
    "<hr/>\n",
    "Both approaches will not work for feature distribution that has heavy tail. The solution would be replacing the feature with it\\'s logarithm,\n",
    "bucketizing the feature, etc.\n",
    "<hr/>\n",
    "As we are transforming the data,  we need to reverse the transformation to get the actual values\n",
    "</td>\n",
    " <td>\n",
    " from sklearn.preprocessing import MinMaxScaler\n",
    "<br/>\n",
    "min_max_scaler = <b>MinMaxScaler(feature_range=(-1,1))</b>\n",
    "<br/>\n",
    "housing_num_min_max_scaled = min_max_scaler.<b>fit_transform(housing_num)</b>\n",
    "<hr/>\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "<br/>\n",
    "std_scaler = StandardScaler()\n",
    "<br/>\n",
    "housing_num_std_scaled =  std_scaler.fit_transform(housing_num)\n",
    "<hr/>\n",
    "target_scaler = StandardScaler()\n",
    "<br/>\n",
    "...\n",
    "<br/>\n",
    "...\n",
    "<br/>\n",
    "scaled_predictions = model.predict(some_new_data)\n",
    "<br/>\n",
    " predictions = traget_scaler.<b>inverse_transform</b>(scaled_predictions)\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    " 7 \n",
    "</td>\n",
    " <td>\n",
    " Custom transformer (optional)\n",
    "</td>\n",
    " <td>\n",
    " This is an optional step where feature scaling with standard transformers is not sufficient.\n",
    "<hr>\n",
    " We can create a basic custom transformer using the function transformer. \n",
    " For replacing heavy-tailed distribution with its logarithm, we can use the function transformer.\n",
    "<hr>\n",
    "If we would like to tranformer to be trainable: learning some parameter in the fit() and using them later in the transform() just like other standard tranformers,\n",
    "We need to write a custom class\n",
    "TransformerMixin parent class brings the fir_transform method so we do not need to specifically write here\n",
    "\n",
    "</td>\n",
    " <td>\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "<br/>\n",
    "log_transformer = <b>FunctionTransformer(np.log, inverse_func=np.exp)</b> \n",
    "<br/>\n",
    "log_transformer.transform(housing['population'])\n",
    "<hr>\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "<br/>\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "<br/>\n",
    "<br/>\n",
    "<b>class custom_class_name(BaseEstimator, TransformerMixin):</b>\n",
    "<br/>\n",
    "<br/>\n",
    "    def __init__(self, with_mean=True):\n",
    "        <br/>   \n",
    "        self.with_mean=True\n",
    "<br/>\n",
    "<br/>\n",
    "    <b>def fit(self, X, y=None):</b>\n",
    "        <br/>   \n",
    "        return self \n",
    "<br/>\n",
    "<br/>\n",
    "    <b>def transform(self, X):</b>\n",
    "        <br/>   \n",
    "        return transformed_data\n",
    "<br/>   \n",
    "<br/>  \n",
    "custom_class_name_clone = custom_class_name()\n",
    "<br/>\n",
    "<br/>\n",
    "scaled = custom_class_name_clone.fit_transform(df)\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    " 8\n",
    "</td>\n",
    " <td>\n",
    "Transformation pipelines\n",
    "</td>\n",
    " <td>\n",
    " Scikit-learn provides the pipeline class to help with sequence of transformations, mainly for numerical data.\n",
    "<hr>\n",
    "So far we've dealt with numerical & categorical features separately. We can combine them using the ColumnTransformer class \n",
    "</td>\n",
    " <td>\n",
    "from sklearn.pipeline import Pipeline\n",
    "<br/>\n",
    "<b>num_pipeline = Pipeline([</b>\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"standardize\", StandardScaler()),\n",
    "])\n",
    "<br/>\n",
    "housing_num_prepared = num_pipeline.<b>fit_transform(housing_num)</b>\n",
    "<br/>\n",
    "df_test = pd.DataFrame(housing_num_prepared, columns=num_pipeline.get_feature_names_out(), index = housing_num.index)\n",
    "<hr>\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "<br/>\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "<br/>\n",
    "from sklearn.compose import ColumnTransformer\n",
    "<br/>\n",
    "<br/>\n",
    "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
    "               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
    "<br/>\n",
    "<br/>\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "<br/>\n",
    "<br/>\n",
    "<b>num_pipeline = Pipeline</b>([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"standardize\", StandardScaler())\n",
    "])\n",
    "<br/>\n",
    "<br/>\n",
    "<b>cat_pipeline = Pipeline</b>([\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"one_hot_encoding\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "<br/>\n",
    "<br/>\n",
    "<b>preprocessing = ColumnTransformer</b>([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs)\n",
    "])\n",
    "<br/>\n",
    "<br/>\n",
    "housing_prepared = <b>preprocessing.fit_transform(housing)</b>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    " 9 \n",
    "</td>\n",
    " <td>\n",
    "Train a model by adding to the training pipeline & calling the fit method\n",
    "</td>\n",
    " <td>\n",
    "We already have the trnasformation pipeline and we can add model training to the pipeline. Then fir on the data\n",
    "</td>\n",
    " <td>\n",
    "\n",
    "<b>tree_reg = Pipeline([</b>\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('decision_tree_regressor',DecisionTreeRegressor(random_state=42))\n",
    "])\n",
    "<b>tree_reg.fit</b>(housing, housing_labels)\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    " 10\n",
    "</td>\n",
    " <td>\n",
    "Efficient evaluation using cross validation (Optinal)\n",
    "</td>\n",
    " <td>\n",
    "We can better evaluate the model using k fold cross validation. In this, training set is randomly splits into 10 nooverlpaping folds is k=10.\n",
    "Then it train & evalaute the decision tree model 10 times, picking a different fold for evaluation every time and using the other 9 folds for training. \n",
    "The result is an array containing the 10 evaluation scores.\n",
    "<img src='images/cross_validation.png' width=250/>\n",
    "\n",
    "</td>\n",
    " <td>\n",
    "from sklearn.model_selection import cross_val_score\n",
    "<br/>\n",
    "tree_rmse = <b>-cross_val_score</b>(tree_reg, housing, housing_labels, scoring=\"neg_root_mean_squared_error\", <b>cv=10</b>)\n",
    "<br/>\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    " 11 \n",
    "</td>\n",
    " <td>\n",
    " Finetune the model (Optional)\n",
    "</td>\n",
    " <td>\n",
    " Normally we need to manually tune the hyperparameters to get the best fit model. Scikit-learn provides tuning methods along with cross-validation: Grid search cv and randomized search cv\n",
    "GridsearchCV is used when the search space is realtively small and randomizedSearchCV is preferred when the search space is large or continuous.\n",
    "\n",
    "    </td>\n",
    " <td>\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "<br/>\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "<br/>\n",
    "<br/>\n",
    "<b>full_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('random_forest', RandomForestRegressor(random_state=42))\n",
    "])</b>\n",
    "<br/>\n",
    "<br/>\n",
    "# Hyperparameters of random_forest: search space has only two items\n",
    "<br/>\n",
    "<b>param_grid = [</b>\n",
    "    {'random_forest__max_features': [4, 6, 8]}\n",
    "]\n",
    "<br/>\n",
    "<br/>\n",
    "grid_search = <b>GridSearchCV</b>(full_pipeline, param_grid, cv=3, scoring='neg_root_mean_squared_error')\n",
    "<br/>\n",
    "<br/>\n",
    "grid_search.<b>fit</b>(housing, housing_labels)\n",
    "\n",
    "<hr>\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "<br/>\n",
    "from scipy.stats import randint \n",
    "<br/>\n",
    "<br/>\n",
    "<b>full_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('random_forest', RandomForestRegressor(random_state=42))\n",
    "])</b>\n",
    "<br/>\n",
    "<br/>\n",
    "<b>param_distribs = {'random_forest__max_features': randint(low=2, high=4)}</b>\n",
    "<br/>\n",
    "<br/>\n",
    "rnd_search = <b>RandomizedSearchCV(</b>\n",
    "    full_pipeline, <b>param_distributions=param_distribs, n_iter=1, </b>cv=3,\n",
    "    scoring='neg_root_mean_squared_error', random_state=42)\n",
    "<br/>\n",
    "<br/>\n",
    "rnd_search.fit(housing, housing_labels)\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    " 12\n",
    "</td>\n",
    " <td>\n",
    " Analysing the best models and their errors (Optional)\n",
    "</td>\n",
    " <td>\n",
    "We can analyse the feature's importance so that we can drop and keep the features which are less or more important.\n",
    "</td>\n",
    " <td>\n",
    "final_model = rnd_search.best_estimator_\n",
    "<br/>\n",
    "<br/>\n",
    "feature_importances = final_model[\"random_forest\"].feature_importances_\n",
    "<br/>\n",
    "<br/>\n",
    "sorted(zip(feature_importances, final_model['preprocessing'].get_feature_names_out()), reverse=True)\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    " 13\n",
    "</td>\n",
    " <td>\n",
    " Evaluate your system on the test set\n",
    "</td>\n",
    " <td>\n",
    "It helps to identify how the system will perform on unseen data.\n",
    "</td>\n",
    " <td>\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "<br/>\n",
    "<br/>\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "<br/>\n",
    "<br/>\n",
    "final_predictions = final_model.predict(X_test)\n",
    "<br/>\n",
    "<br/>\n",
    "final_rmse = mean_squared_error(y_test, final_predictions, squared=False)\n",
    "print(final_rmse)\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    " 14\n",
    "</td>\n",
    " <td>\n",
    " Saving and loading the model\n",
    "</td>\n",
    " <td>\n",
    "\n",
    "</td>\n",
    " <td>\n",
    "import joblib\n",
    "<br/>\n",
    "    <br/>\n",
    "# Save the final model\n",
    "    <br/>\n",
    "joblib.dump(final_model, \"my_california_housing_model.pkl\")\n",
    "    <br/>\n",
    "    <br/>\n",
    "\n",
    "# Load the saved model\n",
    "    <br/>\n",
    "final_model_reloaded = joblib.load(\"my_california_housing_model.pkl\")\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f9cd9-4ac7-428b-a620-90b2ac495eea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
