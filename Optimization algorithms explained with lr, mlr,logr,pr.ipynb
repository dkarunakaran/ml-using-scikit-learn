{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e302bf3f-4b38-4d43-ad5e-843a73b93dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Optimization is the problem of finding a set of inputs to an objective function that results in a maximum or minimum function evaluation.\n",
       "\n",
       "<h3>Gradient Descent</h3>\n",
       "<a href=\"https://medium.com/intro-to-artificial-intelligence/gradient-descent-algorithm-explained-with-linear-regression-example-ff6b5491fdb9\">Gradient descent optimization explained using linear regression example</a>\n",
       "The first image below explain the goal of the GD to minimize the cost/loss function by optimising the learning parameters(weights & biases). \n",
       "The second & third images explain the importance of right learning rate hyper parameter. \n",
       "<img src=\"images/gd1.jpg\" width=400 />\n",
       "<img src=\"images/gd2.jpg\" width=400 />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "Optimization is the problem of finding a set of inputs to an objective function that results in a maximum or minimum function evaluation.\n",
    "\n",
    "<h3>Gradient Descent</h3>\n",
    "<a href=\"https://medium.com/intro-to-artificial-intelligence/gradient-descent-algorithm-explained-with-linear-regression-example-ff6b5491fdb9\">Gradient descent optimization explained using linear regression example</a>\n",
    "The first image below explain the goal of the GD to minimize the cost/loss function by optimising the learning parameters(weights & biases). \n",
    "The second & third images explain the importance of right learning rate hyper parameter. \n",
    "<img src=\"images/gd1.jpg\" width=400 />\n",
    "<img src=\"images/gd2.jpg\" width=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56a2e21-3f13-4cc7-b114-4a3fe6e3f4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "There is a pitafall in GD where algorithm may fall in local optimum thinking of it's a global optimum.\n",
       "<img src=\"images/gd3.jpg\" width=400 />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html \n",
    "There is a pitafall in GD where algorithm may fall in local optimum thinking of it's a global optimum.\n",
    "<img src=\"images/gd3.jpg\" width=400 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02be4b5f-dc57-473e-9906-4a577f60c683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The gradient is a fancy word for derivative, or the rate of change of a function. \n",
       "In a univariate function(i.e. one feature(x) variable and a target(y) variable), it is computed by \n",
       "Gradient =  Change in Y/Change in X\n",
       " \t \t\n",
       "<img src=\"images/slope.svg\" width=400 />\n",
       "\n",
       "f(x) (i.e. a function) gives the y, we can say that gradient/derivative, in mathematics, is the rate of change of a function with respect to a variable. \n",
       "\n",
       "In higher dimension/ multivariate function, we have more than one variable, we need to compute the partial derivative of f(x) w.r.t each variable.\n",
       "Then gradient in the higher dimension is a vector of all partial derivatives. \n",
       "\n",
       "<br/>\n",
       "<br/>\n",
       "Consider the multivariable function f(x, y) = xy²+x³. We can find partial derivatives of the function that is derivatives of function wrt to each variable x and y(a bit of calculus knowledge is required to compute the partial derivatives).\n",
       "<img src=\"images/pd1.png\" width=400 />\n",
       "<img src=\"images/pd2.png\" width=400 />\n",
       "Gradient of f(x) is\n",
       "<img src=\"images/gradient_hd.png\" width=400 />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "The gradient is a fancy word for derivative, or the rate of change of a function. \n",
    "In a univariate function(i.e. one feature(x) variable and a target(y) variable), it is computed by \n",
    "Gradient =  Change in Y/Change in X\n",
    " \t \t\n",
    "<img src=\"images/slope.svg\" width=400 />\n",
    "\n",
    "f(x) (i.e. a function) gives the y, we can say that gradient/derivative, in mathematics, is the rate of change of a function with respect to a variable. \n",
    "\n",
    "In higher dimension/ multivariate function, we have more than one variable, we need to compute the partial derivative of f(x) w.r.t each variable.\n",
    "Then gradient in the higher dimension is a vector of all partial derivatives. \n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "Consider the multivariable function f(x, y) = xy²+x³. We can find partial derivatives of the function that is derivatives of function wrt to each variable x and y(a bit of calculus knowledge is required to compute the partial derivatives).\n",
    "<img src=\"images/pd1.png\" width=400 />\n",
    "<img src=\"images/pd2.png\" width=400 />\n",
    "The gradient of f(x) is\n",
    "<img src=\"images/gradient_hd.png\" width=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1bb9ca3-69af-4ca6-997f-ff5c16c01842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<b>Training a model means searching for a combination of model  parameters that minimizes a cost function. It is a search in the model's parameter space.\n",
       "The more parameters a model has, the more dimensions in this space have, and the harder the search.</b>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<b>Training a model means searching for a combination of model  parameters that minimizes a cost function. It is a search in the model's parameter space.\n",
    "The more parameters a model has, the more dimensions in this space have, and the harder the search.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2b63d463-d9f7-4945-94e8-584b2819bf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> Implementation of simple linear regression</h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3> Implementation of simple linear regression</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88758051-e8d5-4984-8f60-c637562cc8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The linear regression fits the best matching line to the data for the prediction. \n",
       "The formula for the line is y= wx+b. here we have one feature x and the outcome is y.\n",
       "so f(x) = mx+b, basically, here we need to find parameters, w & b, for fitting the line to the data. \n",
       "To compute the best parameter, we can check by adding different values to w & b and see the predicted y(y=wx+b) \n",
       "is similar to the ground truth.\n",
       "The checking can be done through a cost function, a simple cost function can be a squared error(i.e. (y^ - y)2). \n",
       "If we have m examples, we can take the average of the squared value. J here is the cost function with parameter w & b\n",
       "<img src=\"images/cost_lr2.png\" width=400 />\n",
       "In the gradient descent algorithm, we are trying to minimize the cost function by optimising the parameter. \n",
       "Each partial derivative(we call it gradient, even though the vector of partial derivatives is gradient) gives the rate of change and direction required to maximise the cost w.r.t the parameter(As the partial derivatives give the +ve direction to maximise). \n",
       "In order to compute the minimum, we need to reverse the direction(just hold on to that thought there).\n",
       "so, adding the current value of the parameter with the gradient of that parameter gives the next parameter value that maximise the cost. \n",
       "As we need to go in the reverse direction so instead of adding, we subtract.\n",
       "<img src=\"images/cost_lr1.png\" width=400 />\n",
       "Algorithm of gradient descent for LR:\n",
       "<img src=\"images/cost_lr3.jpg\" width=400 />\n",
       "The gradient can be computed using the following formula:\n",
       "<img src=\"images/cost_lr4.png\" width=400 />\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "The linear regression fits the best matching line to the data for the prediction. \n",
    "The formula for the line is y= wx+b. here we have one feature x and the outcome is y.\n",
    "so f(x) = mx+b, basically, here we need to find parameters, w & b, for fitting the line to the data. \n",
    "To compute the best parameter, we can check by adding different values to w & b and see the predicted y(y=wx+b) \n",
    "is similar to the ground truth.\n",
    "The checking can be done through a cost function, a simple cost function can be a squared error(i.e. (y^ - y)2). \n",
    "If we have m examples, we can take the average of the squared value. J here is the cost function with parameter w & b\n",
    "<img src=\"images/cost_lr2.png\" width=400 />\n",
    "In the gradient descent algorithm, we are trying to minimize the cost function by optimising the parameter. \n",
    "Each partial derivative(we call it gradient, even though the vector of partial derivatives is gradient) gives the rate of change and direction required to maximise the cost w.r.t the parameter(As the partial derivatives give the +ve direction to maximise). \n",
    "In order to compute the minimum, we need to reverse the direction(just hold on to that thought there).\n",
    "so, adding the current value of the parameter with the gradient of that parameter gives the next parameter value that maximise the cost. \n",
    "As we need to go in the reverse direction so instead of adding, we subtract.\n",
    "<img src=\"images/cost_lr1.png\" width=400 />\n",
    "Algorithm of gradient descent for LR:\n",
    "<img src=\"images/cost_lr3.jpg\" width=400 />\n",
    "The gradient can be computed using the following formula:\n",
    "<img src=\"images/cost_lr4.png\" width=400 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79e55573-b150-4fa7-8a57-79cd0f3d8542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e17bd40-5c79-4f60-9fc4-98b3313d4c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  \n",
       "0        -122.23  \n",
       "1        -122.22  \n",
       "2        -122.24  \n",
       "3        -122.25  \n",
       "4        -122.25  \n",
       "...          ...  \n",
       "20635    -121.09  \n",
       "20636    -121.21  \n",
       "20637    -121.22  \n",
       "20638    -121.32  \n",
       "20639    -121.24  \n",
       "\n",
       "[20640 rows x 8 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "322bcd96-6c0d-48ec-bd04-dcee678045f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        4.526\n",
       "1        3.585\n",
       "2        3.521\n",
       "3        3.413\n",
       "4        3.422\n",
       "         ...  \n",
       "20635    0.781\n",
       "20636    0.771\n",
       "20637    0.923\n",
       "20638    0.847\n",
       "20639    0.894\n",
       "Name: MedHouseVal, Length: 20640, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f7805ad4-1bc2-4c18-a5d1-0b70e85bd77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      41.0\n",
       "1      21.0\n",
       "2      52.0\n",
       "3      52.0\n",
       "4      52.0\n",
       "       ... \n",
       "995    11.0\n",
       "996    25.0\n",
       "997    22.0\n",
       "998    17.0\n",
       "999    12.0\n",
       "Name: HouseAge, Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For simple linear regression, we only consider one feature\n",
    "X = housing.data[\"HouseAge\"][:1000]\n",
    "Y = housing.target[:1000]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "aacf1c0a-6884-42a1-b7c4-e53a694f11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w, b, X, Y, iterations = 10, learning_rate = 0.001):\n",
    "    w = 0\n",
    "    b = 0\n",
    "    m = X.shape[0]\n",
    "    # Estimation of optimal parameters \n",
    "    for k in range(iterations):\n",
    "        sum_for_w = 0\n",
    "        sum_for_b = 0\n",
    "        # We are computing the gradient w.r.t w & b\n",
    "        for i in range(m):\n",
    "            y_pred_i = w*X[i]+b \n",
    "            y_gt_i = Y[i]\n",
    "            sum_for_w += (y_pred_i-y_gt_i)* X[i]\n",
    "            sum_for_b += (y_pred_i-y_gt_i)\n",
    "        \n",
    "        grad_w = sum_for_w/m\n",
    "        grad_b = sum_for_b/m\n",
    "        w = w - (learning_rate* grad_w)    \n",
    "        b = b- (learning_rate * grad_b)\n",
    "\n",
    "    return w, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f825ec9f-f293-451b-8469-f65191640e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.04841381460154124, 0.0038357696060325043)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(w,b,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c36aacbd-331f-4b6c-be2f-e9a9085df147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HouseAge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     HouseAge\n",
       "0        41.0\n",
       "1        21.0\n",
       "2        52.0\n",
       "3        52.0\n",
       "4        52.0\n",
       "..        ...\n",
       "995      11.0\n",
       "996      25.0\n",
       "997      22.0\n",
       "998      17.0\n",
       "999      12.0\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "51e4398d-aa77-43ff-82ca-66d365c3bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "num_attribs = [\"HouseAge\"]\n",
    "cat_attribs = []\n",
    "num_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"standardize\", StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"one_hot_encoding\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "feature_engineering = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs)\n",
    "])\n",
    "\n",
    "linear_reg  = Pipeline([\n",
    "    (\"feature_engineering\", feature_engineering),\n",
    "    (\"model\", linear_reg)\n",
    "])\n",
    "\n",
    "reg = linear_reg.fit(X.to_frame(),Y) #X.to_frame() it expects a 2d array(n_samples, n_features) that is df not the series(1d array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c9b49835-ed05-4fa3-ba6d-9635529e3cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.445421830407677"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg['model'].coef_\n",
    "reg['model'].intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "566d7f29-e550-4220-96fa-3bb838c46231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4> Implementation of multivariate linear regression</h4>\n",
       "Univariate linear regression means we have only one feature in the input, x. \n",
       "For instance, house prices prediction example in the previous article had only one feature in x which is the age of the house. \n",
       "In the case of multiple linear regression, there could be n number of features. \n",
       "For instance, the number of rooms, size of the house etc. \n",
       "In other words, we can say each training example will have n number of columns as opposed to a single value for the univariate linear regression.\n",
       "<br/>\n",
       "So we can represent the multiple features in a vector.\n",
       "<img src=\"images/mult_lr1.png\" width=400 />   \n",
       "So, we can represent the multiple linear regression in the below formula:\n",
       "<img src=\"images/mult_lr2.png\" width=400 />  \n",
       "As it is clear from the above picture, there is a vector of weights that need to be computed along with b to solve the problem.\n",
       "<img src=\"images/mult_lr3.png\" width=400 />  \n",
       "\n",
       "We can say that there are j number of w and a bias parameters.\n",
       "<img src=\"images/mult_lr4.png\" width=400 />  \n",
       "\n",
       "So, now we can expand the above algorithm by filling the gradient with corresponding equation.\n",
       "<img src=\"images/mult_lr5.png\" width=400 />  \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3> Implementation of multivariate linear regression</h3>\n",
    "Univariate linear regression means we have only one feature in the input, x. \n",
    "For instance, the house price prediction example in the previous article had only one feature in x which is the age of the house. \n",
    "In the case of multiple linear regression, there could be n number of features. \n",
    "For instance, the number of rooms, size of the house etc. \n",
    "In other words, we can say each training example will have n number of columns as opposed to a single value for the univariate linear regression.\n",
    "<br/>\n",
    "So we can represent the multiple features in a vector.\n",
    "<img src=\"images/mult_lr1.png\" width=400 />   \n",
    "So, we can represent the multiple linear regression in the below formula:\n",
    "<img src=\"images/mult_lr2.png\" width=400 />  \n",
    "As it is clear from the above picture, there is a vector of weights that need to be computed along with b to solve the problem.\n",
    "<img src=\"images/mult_lr3.png\" width=400 />  \n",
    "\n",
    "We can say that there are j number of w and a bias parameters.\n",
    "<img src=\"images/mult_lr4.png\" width=400 />  \n",
    "\n",
    "So, now we can expand the above algorithm by filling the gradient with the corresponding equation.\n",
    "<img src=\"images/mult_lr5.png\" width=400 />  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9dcff1b7-4d9d-46aa-9996-e8924a927cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  \n",
       "0        -122.23  \n",
       "1        -122.22  \n",
       "2        -122.24  \n",
       "3        -122.25  \n",
       "4        -122.25  \n",
       "...          ...  \n",
       "20635    -121.09  \n",
       "20636    -121.21  \n",
       "20637    -121.22  \n",
       "20638    -121.32  \n",
       "20639    -121.24  \n",
       "\n",
       "[20640 rows x 8 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e5cfca04-59b5-4825-be78-59b798cff1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing.data[:100]\n",
    "Y = housing.target[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "212562b0-5d46-4ae1-94d1-1d875d9d9de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2.0096</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2.294016</td>\n",
       "      <td>1.066294</td>\n",
       "      <td>3469.0</td>\n",
       "      <td>1.493328</td>\n",
       "      <td>37.80</td>\n",
       "      <td>-122.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2.8345</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.894915</td>\n",
       "      <td>1.127966</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>1.735593</td>\n",
       "      <td>37.82</td>\n",
       "      <td>-122.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2.0062</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.681319</td>\n",
       "      <td>1.175824</td>\n",
       "      <td>202.0</td>\n",
       "      <td>2.219780</td>\n",
       "      <td>37.81</td>\n",
       "      <td>-122.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.2185</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.945600</td>\n",
       "      <td>1.016000</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1.619200</td>\n",
       "      <td>37.82</td>\n",
       "      <td>-122.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2.6104</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.707143</td>\n",
       "      <td>1.107143</td>\n",
       "      <td>1838.0</td>\n",
       "      <td>1.875510</td>\n",
       "      <td>37.82</td>\n",
       "      <td>-122.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0   8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1   8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2   7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3   5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4   3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "..     ...       ...       ...        ...         ...       ...       ...   \n",
       "95  2.0096      36.0  2.294016   1.066294      3469.0  1.493328     37.80   \n",
       "96  2.8345      31.0  3.894915   1.127966      2048.0  1.735593     37.82   \n",
       "97  2.0062      29.0  3.681319   1.175824       202.0  2.219780     37.81   \n",
       "98  1.2185      22.0  2.945600   1.016000      2024.0  1.619200     37.82   \n",
       "99  2.6104      37.0  3.707143   1.107143      1838.0  1.875510     37.82   \n",
       "\n",
       "    Longitude  \n",
       "0     -122.23  \n",
       "1     -122.22  \n",
       "2     -122.24  \n",
       "3     -122.25  \n",
       "4     -122.25  \n",
       "..        ...  \n",
       "95    -122.26  \n",
       "96    -122.26  \n",
       "97    -122.26  \n",
       "98    -122.26  \n",
       "99    -122.26  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f875bb87-1fc5-4a4e-be8d-65ed5d5dc183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "We can simply calculate the multivariate linear regression using linear Algerbra & Numpy.\n",
       "<a href=\"https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression\">Based on linear algebra</a>, \n",
       "We can compute the weight vector, w using the following equation. Tn the equation beta means w.\n",
       "<img src=\"images/mult_lr_4.png\" width=400 />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "We can simply calculate the multivariate linear regression using linear Algerbra & Numpy.\n",
    "<a href=\"https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression\">Based on linear algebra</a>, \n",
    "We can compute the weight vector, w using the following equation. Tn the equation beta means w.\n",
    "<img src=\"images/mult_lr_4.png\" width=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bc47c8bb-270a-418a-b6e1-6b8bcb5823d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can compute the above equation using numpy: https://medium.com/analytics-vidhya/multiple-linear-regression-from-scratch-using-only-numpy-98fc010a1926\n",
    "\n",
    "def normal_equation(X, Y):\n",
    "    return np.dot(np.linalg.inv(np.dot(X.T,X)),np.dot(X.T,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "15a27ebd-615e-4328-a490-7366bc160a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, w):\n",
    "    return np.dot(X_test, w) # y= W.X and it's a simple equation without n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e506f44b-24e1-4a7f-be78-44c9903cfba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.36813722e-01 -3.41972170e-03 -1.78045034e-01 -8.37655209e-03\n",
      "  3.14047110e-05  1.07360358e-01  1.10133347e+01  3.39516108e+00]\n"
     ]
    }
   ],
   "source": [
    "W = normal_equation(X, Y) # W vector\n",
    "predict(X[:1], W)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8cd15736-852d-4137-9772-bf598243ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "num_attribs = X.columns.to_list()\n",
    "cat_attribs = []\n",
    "num_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"standardize\", StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"one_hot_encoding\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "feature_engineering = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs)\n",
    "])\n",
    "\n",
    "linear_reg  = Pipeline([\n",
    "    (\"feature_engineering\", feature_engineering),\n",
    "    (\"model\", linear_reg)\n",
    "])\n",
    "\n",
    "reg = linear_reg.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "acb99762-eab0-4f5f-aa9c-4d1cf32c34b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19059518, -0.00455346,  0.08597936, -0.12854725, -0.07961133,\n",
       "        0.08922712, -0.14322397,  0.57821715])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg['model'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2821d660-0010-48db-8b65-b658f0fe9332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> Implementation of ploynomial regression</h3>\n",
       "Not all data can be fit with a line. So we need to add nonlinearity (curvature) to fit the data. Polynomial regression can fit a non-linear (curvature) on data.\n",
       "The equation of polynomial regression that fits the cubic function on the data with one feature is:\n",
       "f_w,b(x) = w1x+w2^x2+w3x^3\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3> Implementation of ploynomial regression</h3>\n",
    "Not all data can be fit with a line. So we need to add nonlinearity (curvature) to fit the data. Polynomial regression can fit a non-linear (curvature) on data.\n",
    "The equation of polynomial regression that fits the cubic function on the data with one feature is:\n",
    "f_w,b(x) = w1x+w2^x2+w3x^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "db0a1a5f-80a7-4261-856a-bd4a6cb8e4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;poly_features&#x27;,\n",
       "                 PolynomialFeatures(degree=3, include_bias=False)),\n",
       "                (&#x27;model&#x27;, LinearRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;poly_features&#x27;,\n",
       "                 PolynomialFeatures(degree=3, include_bias=False)),\n",
       "                (&#x27;model&#x27;, LinearRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PolynomialFeatures</label><div class=\"sk-toggleable__content\"><pre>PolynomialFeatures(degree=3, include_bias=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('poly_features',\n",
       "                 PolynomialFeatures(degree=3, include_bias=False)),\n",
       "                ('model', LinearRegression())])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use the linear regression in the sklearn to implement the polynomial regression. \n",
    "# In order to do that, we need to transform the data into polynomial features.\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "poly_reg = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree =3, include_bias=False)),\n",
    "     (\"model\", LinearRegression())\n",
    "])\n",
    "poly_reg.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "22ae7191-8cae-452d-9cb5-9c8fe8a835d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.63168747e-03,  5.09294363e-02,  4.36225686e-02,  8.51923414e-03,\n",
       "        1.85154563e-02,  6.93506568e-03, -1.30729803e-03, -1.26615962e-04,\n",
       "        4.24513619e-02,  2.88485959e-01,  8.06101347e-02,  1.10274891e-02,\n",
       "        7.32894415e+00,  2.69298218e-02,  1.02108977e-01, -3.53188595e-01,\n",
       "        2.97870090e+00,  1.30419946e+00,  2.48381800e-01, -1.56089634e+00,\n",
       "        2.89140055e-01,  9.29674007e-01, -3.11771570e+00,  2.71174000e-01,\n",
       "        4.96719057e-02,  5.35980828e+00,  1.04776491e-01,  8.24512636e-01,\n",
       "       -2.66719406e+00,  8.01007938e-03,  1.78437916e+00,  5.35027786e-03,\n",
       "        1.32580825e-01, -5.25954621e-01, -3.88008156e-02, -4.67449690e+00,\n",
       "        1.71965476e-02, -9.60968336e-01,  3.00178796e-02,  1.25948247e-01,\n",
       "       -4.24995153e-01, -4.70031119e-02,  7.82660749e-02,  1.55756447e-02,\n",
       "        4.06961136e-02, -3.88910832e-02, -9.20892564e-01,  3.14315338e-01,\n",
       "       -4.45572456e-04,  1.14127372e+00,  4.22934202e+00,  1.28194902e+00,\n",
       "        1.12898305e-03, -3.60224498e-02,  7.63705882e-01,  2.03799597e-04,\n",
       "        6.36916911e-01,  2.22393554e+00,  7.08424147e-01,  1.11744585e+00,\n",
       "       -3.98408098e+00, -9.33629270e-05, -1.16812483e+00,  1.92738910e+00,\n",
       "        5.76646094e-01, -2.38472984e+00,  1.08324917e-02,  1.12111437e+00,\n",
       "       -3.44169004e-01,  9.92690055e-02,  1.98154574e-06,  6.95475690e-04,\n",
       "       -5.13206187e-02,  4.42445449e-02, -2.19171874e-02,  6.36761354e-02,\n",
       "        2.85416167e-01, -3.45617723e-01,  6.10577678e-01,  2.25469579e-01,\n",
       "        3.10511773e-04, -3.26805631e-03,  2.01665167e-02, -1.31997690e-05,\n",
       "       -2.36713576e-02, -6.35241823e-04,  2.40712901e-02,  1.92461863e-02,\n",
       "       -3.00690448e-01, -1.80971361e-04, -2.64569960e-02,  7.76867835e-02,\n",
       "        2.92112589e-02, -1.94463128e+00,  4.55557900e-03,  9.00399164e-01,\n",
       "       -6.16029121e-01, -1.70688736e-01,  3.11494022e-07,  6.04064928e-04,\n",
       "        1.18298027e-02, -9.06300368e-03, -2.11531462e-01, -1.61078877e-01,\n",
       "       -5.35242555e-02,  1.19785095e-01,  1.19260111e-01, -2.30457057e-03,\n",
       "       -1.93054955e-01, -2.54422137e-02,  2.01024559e-03,  1.11938894e-01,\n",
       "       -1.63793540e-01, -3.04570582e-02, -3.54344219e+00, -3.52692922e-02,\n",
       "        9.71528456e+00,  1.04330847e-01, -2.66243975e-01, -7.18631381e-06,\n",
       "       -6.66390703e-03, -1.39265274e-01,  2.92852097e-04, -6.82380264e-01,\n",
       "       -3.33477701e-02, -1.28246648e-02,  2.16232655e-01, -3.39867301e-01,\n",
       "       -1.53101540e-01, -3.84868891e-01,  2.30823704e-01,  8.18861761e-01,\n",
       "       -1.19139299e-01,  1.70899299e-01,  3.55366042e-05,  5.00945810e-02,\n",
       "        9.45514979e-01,  3.13354575e-01,  4.05606031e+00, -3.36220888e-01,\n",
       "        1.26903042e+00, -2.10917298e+00,  3.21213635e+00,  1.22168906e+00,\n",
       "        4.67316236e-09,  9.17719412e-06,  2.99301581e-04, -2.24231558e-04,\n",
       "        2.04316837e-03,  2.08405637e-02, -3.11503934e-02, -8.01117700e-01,\n",
       "       -4.84695792e-01, -8.11406724e-02, -1.34244924e-01, -1.21038178e+00,\n",
       "       -4.41423725e-01, -2.53080031e-01,  3.92085336e-01,  1.50081761e-01,\n",
       "        2.78513685e-01, -2.46800553e-01, -1.74059207e-01, -3.22070711e-02])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_reg['model'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "26f08be1-dbc7-4a77-ad7f-f8d381a9cbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "Polynomial regression captures only a certain amount of curvature. Higher order terms in polynomial regression to get the more amount curavture will result in undesirable result. \n",
       "The superioe approach is using splines. The splines are series of polynomial segements strung together, joining knots.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "Polynomial regression captures only a certain amount of curvature. Higher order terms in polynomial regression to get the more amount curavture will result in undesirable result. \n",
    "The superioe approach is using splines. The splines are series of polynomial segements strung together, joining knots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1547852a-3030-4768-a6e1-8bf677816bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Different types of gradient descents</h3>\n",
       "<h4>Batch gradient descents</h4>\n",
       "In Batch Gradient Descent, all the training data is taken into consideration to take a single step. \n",
       "We take the average of the gradients of all the training examples and then use that mean gradient to update our parameters. \n",
       "So that’s just one step of gradient descent in one epoch.\n",
       "<h4>Stochastic Gradient Descent</h4>\n",
       "n Batch Gradient Descent we were considering all the examples for every step of Gradient Descent. But what if our dataset is very huge. \n",
       "Deep learning models crave data. The more the data the more chances of a model to be good. \n",
       "Suppose our dataset has 5 million examples, then just to take one step the model will have to calculate the gradients of \n",
       "all the 5 million examples. This does not seem an efficient way. To tackle this problem we have Stochastic Gradient Descent. \n",
       "In Stochastic Gradient Descent (SGD), we consider just one example at a time to take a single step. We do the following steps \n",
       "in one epoch for SGD:\n",
       "<br/>\n",
       "1. Take an example\n",
       "    <br/>\n",
       "    2. Calculate its gradient\n",
       "<br/>\n",
       "    3. Use the gradient we calculated in step 3 to update the weights\n",
       "<br/>\n",
       "    4. Repeat steps 1–4 for all the examples in the training dataset\n",
       "<br/>\n",
       "    <br/>\n",
       "\n",
       "Since we are considering just one example at a time the cost will fluctuate over the training examples and it will not necessarily decrease. \n",
       "    But in the long run, you will see the cost decreasing with fluctuations.\n",
       "\n",
       "<h4>mini-batch Gradient Descent</h4>\n",
       "    SGD converges faster for larger datasets. But, since in SGD we use only one example at a time, \n",
       "    we cannot implement the vectorized implementation on it. This can slow down the computations.\n",
       "    To tackle this problem, a mixture of Batch Gradient Descent and SGD is used.\n",
       "    We use a batch of a fixed number of training examples which is less than the actual dataset and call it a mini-batch. Doing this helps us achieve the advantages of both the former variants we saw. So, after creating the mini-batches of fixed size, we do the following steps in one epoch:\n",
       "<br/>\n",
       "1. Pick a mini-batch\n",
       "    <br/>\n",
       "2. Calculate the mean gradient of the mini-batch\n",
       "    <br/>\n",
       "3. Use the mean gradient we calculated in step 3 to update the weights\n",
       "    <br/>\n",
       "4. Repeat steps 1–4 for the mini-batches we created\n",
       "    <br/>\n",
       "    <br/>\n",
       "Just like SGD, the average cost over the epochs in mini-batch gradient descent fluctuates \n",
       "        because we are averaging a small number of examples at a time.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h3>Different types of gradient descents</h3>\n",
    "<h4>Batch gradient descents</h4>\n",
    "In Batch Gradient Descent, all the training data is taken into consideration to take a single step. \n",
    "We take the average of the gradients of all the training examples and then use that mean gradient to update our parameters. \n",
    "So that’s just one step of gradient descent in one epoch.\n",
    "<h4>Stochastic Gradient Descent</h4>\n",
    "n Batch Gradient Descent we were considering all the examples for every step of Gradient Descent. But what if our dataset is very huge. \n",
    "Deep learning models crave data. The more the data the more chances of a model to be good. \n",
    "Suppose our dataset has 5 million examples, then just to take one step the model will have to calculate the gradients of \n",
    "all the 5 million examples. This does not seem an efficient way. To tackle this problem we have Stochastic Gradient Descent. \n",
    "In Stochastic Gradient Descent (SGD), we consider just one example at a time to take a single step. We do the following steps \n",
    "in one epoch for SGD:\n",
    "<br/>\n",
    "1. Take an example\n",
    "    <br/>\n",
    "    2. Calculate its gradient\n",
    "<br/>\n",
    "    3. Use the gradient we calculated in step 3 to update the weights\n",
    "<br/>\n",
    "    4. Repeat steps 1–4 for all the examples in the training dataset\n",
    "<br/>\n",
    "    <br/>\n",
    "\n",
    "Since we are considering just one example at a time the cost will fluctuate over the training examples and it will not necessarily decrease. \n",
    "    But in the long run, you will see the cost decreasing with fluctuations.\n",
    "\n",
    "<h4>mini-batch Gradient Descent</h4>\n",
    "    SGD converges faster for larger datasets. But, since in SGD we use only one example at a time, \n",
    "    we cannot implement the vectorized implementation on it. This can slow down the computations.\n",
    "    To tackle this problem, a mixture of Batch Gradient Descent and SGD is used.\n",
    "    We use a batch of a fixed number of training examples which is less than the actual dataset and call it a mini-batch. Doing this helps us achieve the advantages of both the former variants we saw. So, after creating the mini-batches of fixed size, we do the following steps in one epoch:\n",
    "<br/>\n",
    "1. Pick a mini-batch\n",
    "    <br/>\n",
    "2. Calculate the mean gradient of the mini-batch\n",
    "    <br/>\n",
    "3. Use the mean gradient we calculated in step 3 to update the weights\n",
    "    <br/>\n",
    "4. Repeat steps 1–4 for the mini-batches we created\n",
    "    <br/>\n",
    "    <br/>\n",
    "Just like SGD, the average cost over the epochs in mini-batch gradient descent fluctuates \n",
    "        because we are averaging a small number of examples at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e789f6-bf5d-4e4b-b8d6-9bce2fb1c953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Learning curves</h3>\n",
       "\n",
       "If a model performs well on the training data but generalizes poorly on test data, then the model is overfitting. \n",
       "If it performs poorly on both datasets, then the model is underfitting. This is one way to tell if the model is too simple or complex.\n",
       "Another way to tell is to look at the learning curves. It is a plot of the model's training and validation error as a function of the training iteration.\n",
       "<img src=\"images/lc.jpg\" width=400 />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h3>Learning curves</h3>\n",
    "\n",
    "If a model performs well on the training data but generalizes poorly on validation data, then the model is overfitting. \n",
    "If it performs poorly on both datasets, then the model is underfitting. This is one way to tell if the model is too simple or complex.\n",
    "Another way to tell is to look at the learning curves. It plots the model's training and validation error as a function of the training iteration.\n",
    "<img src=\"images/lc.jpg\" width=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894c2aef-598b-4418-ab93-8343d537d1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The above model is undefitting as both traning and validation error is high.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "The above model is underfitting as both training and validation errors are high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc35b4cb-17b0-41fb-b3d9-cfdabeb95f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # to make this code example reproducible\n",
    "m = 100  # number of instances\n",
    "X = 2 * np.random.rand(m, 1)  # column vector\n",
    "y = 4 + 3 * X + np.random.randn(m, 1)  # column vector\n",
    "\n",
    "polynomial_regression = make_pipeline(\n",
    "    PolynomialFeatures(degree=10, include_bias=False),\n",
    "    LinearRegression())\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    polynomial_regression, X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c789eb6-9b74-4ea8-8dcd-02e96896c068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extra code – generates and saves Figure 4–16\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "train_errors = -train_scores.mean(axis=1)\n",
    "valid_errors = -valid_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "plt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Training set size\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid()\n",
    "plt.axis([0, 80, 0, 2.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b420ccf-a161-40d7-a197-dd7781d26f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "In the above graph, the error on the training data is much lower than before. \n",
       "It indicates the model's performance is significantly better on training data than the validation data.\n",
       "<br/>\n",
       "<b>One way to improve an overfitting model is to feed it more training data until the validation error reached the training data or use regularization</b>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "In the above graph, the error on the training data is much lower than before. \n",
    "It indicates the model's performance is significantly better on training data than the validation data.\n",
    "<br/>\n",
    "<b>One way to improve an overfitting model is to feed it more training data until the validation error reaches the training data or use regularization</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23a3e7cd-0d22-4343-a249-464fd3ed3e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Early stopping</h3>\n",
       "\n",
       "Another way to regularize the iterative learning algorithms such as gd is to stop training as soon as the validation reaches the minimum. \n",
       "This is called early stopping.\n",
       "<img src=\"images/early_s.jpg\" width=400 />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3>Early stopping</h3>\n",
    "\n",
    "Another way to regularize the iterative learning algorithms such as gd is to stop training once the validation reaches the minimum. \n",
    "This is called early stopping.\n",
    "<img src=\"images/early_s.jpg\" width=400 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9949f278-7a46-4fcc-be4d-e58e99693417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Bias/Variance trade off</h3>\n",
       "\n",
       "If the data is too small for the model, it is likely to overfit. Same way if the model has many degree of freedom(model is complex such as high degree polynomial model),\n",
       "it is likely to have high variance and thus overfit the training data.\n",
       "\n",
       "<br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3>Bias/Variance trade off</h3>\n",
    "\n",
    "If the data is too small for the model, it is likely to overfit. Same way if the model has many degree of freedom(model is complex such as high degree polynomial model),\n",
    "it is likely to have high variance and thus overfit the training data.\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf484fe-20eb-46b9-a8d8-907e7fc3d3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa95eb-34c6-4a7b-ac3b-40fd0ef4e76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7c876-5bdf-4d20-8c58-ac02c8ab0f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
